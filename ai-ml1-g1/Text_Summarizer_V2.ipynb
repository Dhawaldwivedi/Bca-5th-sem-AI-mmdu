{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNI1XUTgPMwrmvAHVmmh7SJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhawaldwivedi/Bca-5th-sem-AI-mmdu/blob/main/ai-ml1-g1/Text_Summarizer_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNlvJurgCWys",
        "outputId": "3bbf2e74-cd04-4c7f-c59b-27a5f38d5ab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are an expert teaching assistant specializing in Generative AI (GenAI). Your task is to generate a summary of a given paragraph, intended for training or evaluating a Recurrent Neural Network (RNN) based Sequence-to-Sequence (Seq2Seq) model. The summary should serve as a clear and concise representation of the original paragraph's meaning, suitable for students new to GenAI.\n",
            "The summary must meet the following criteria:\n",
            "Content Coverage: Comprehensively cover all essential topics presented in the original paragraph. Ensure it is self-contained and easily understandable for beginners in GenAI.\n",
            "Length Constraint: The summary's length should be approximately 40% of the original paragraph's length.\n",
            "Formatting: Use Markdown for formatting. Emphasize important keywords by using bold text. Format the summary's title as a level 1 heading (e.g., # Seq2Seq Based RNN).\n",
            "Scope Limitation: Strictly avoid introducing any information or concepts not explicitly mentioned in the original paragraph.\n",
            "Evaluation Focus: The summary will be evaluated based on its beginner-friendliness, accuracy in representing the original content, and overall fluency\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Assuming the uploaded file is named \"prompt.md\"\n",
        "filename = \"prompt.md\"\n",
        "\n",
        "# Read the contents\n",
        "with open(filename, \"r\") as file:\n",
        "    prompt_content = file.read()\n",
        "\n",
        "# Display content\n",
        "print(prompt_content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the uploaded file is named \"prompt.md\"\n",
        "filename2 = \"transcript.md\"\n",
        "\n",
        "# Read the contents\n",
        "with open(filename2, \"r\") as file:\n",
        "    transcript_content = file.read()\n",
        "\n",
        "# Display content\n",
        "print(transcript_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlBDRnd6Cd3h",
        "outputId": "7ce22e4a-92e3-402b-bbda-795b740d710d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "What are RNN-based Seq2Seq Models?\n",
            "Hello everyone and Good day to all.\n",
            "In our last session, we introduced the general concept of Sequence-to-Sequence models, consisting of an encoder and a decoder. Historically, and for a long time, Recurrent Neural Networks, or RNNs, particularly LSTMs and GRUs, were the go-to choice for building these Sequence-to-Sequence components. Let's delve into how a Sequence-to-Sequence model works when powered by RNNs.\n",
            "As a recap, recall that RNNs are designed to process sequences by maintaining a hidden state that is updated at each step, incorporating information from the current input and the previous hidden state. This makes them naturally suited for handling sequential data like text.\n",
            "In a Sequence-to-Sequence model with RNNs, the Encoder is typically a multi-layer LSTM network. It reads the input sequence word by word (or token by token). At each time step, the encoder processes the current input token and the hidden state from the previous step to update its internal state. After processing the entire input sequence, ONLY the final hidden state of the encoder is taken as the fixed-size context vector representing the input sentence.\n",
            "Remember, this context vector is the numerical representation of the input sequence. \n",
            " \n",
            "It is then passed to the Decoder. The Decoder is another RNN (usually also an LSTM). It takes this context vector as its initial hidden state. The decoder's task is to generate the output sequence, one token at a time.\n",
            " \n",
            "While this RNN-based architecture was a breakthrough for NMT and other sequence tasks, it has significant drawbacks. The primary issue is the fixed-size context vector. For short sentences, this might be sufficient. But for longer sentences, forcing the encoder to compress all the information into a single vector creates an information bottleneck. The decoder has to rely solely on this single vector, which might not retain all the necessary details, especially those from the beginning of the input sequence. This often leads to performance degradation on longer sequences.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge both files using an f-string\n",
        "merged_output = f\"\"\"# Merged Prompt and Transcript\n",
        "\n",
        "## Prompt\n",
        "{prompt_content}\n",
        "\n",
        "---\n",
        "\n",
        "## Transcript\n",
        "{transcript_content}\n",
        "\"\"\"\n",
        "\n",
        "# Print the final formatted content\n",
        "print(merged_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4s1rpBNIlEQ",
        "outputId": "c2039e64-8df6-4658-c083-b5ffa43aee48"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Merged Prompt and Transcript\n",
            "\n",
            "## Prompt\n",
            "\n",
            "You are an expert teaching assistant specializing in Generative AI (GenAI). Your task is to generate a summary of a given paragraph, intended for training or evaluating a Recurrent Neural Network (RNN) based Sequence-to-Sequence (Seq2Seq) model. The summary should serve as a clear and concise representation of the original paragraph's meaning, suitable for students new to GenAI.\n",
            "The summary must meet the following criteria:\n",
            "Content Coverage: Comprehensively cover all essential topics presented in the original paragraph. Ensure it is self-contained and easily understandable for beginners in GenAI.\n",
            "Length Constraint: The summary's length should be approximately 40% of the original paragraph's length.\n",
            "Formatting: Use Markdown for formatting. Emphasize important keywords by using bold text. Format the summary's title as a level 1 heading (e.g., # Seq2Seq Based RNN).\n",
            "Scope Limitation: Strictly avoid introducing any information or concepts not explicitly mentioned in the original paragraph.\n",
            "Evaluation Focus: The summary will be evaluated based on its beginner-friendliness, accuracy in representing the original content, and overall fluency\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "## Transcript\n",
            "\n",
            "What are RNN-based Seq2Seq Models?\n",
            "Hello everyone and Good day to all.\n",
            "In our last session, we introduced the general concept of Sequence-to-Sequence models, consisting of an encoder and a decoder. Historically, and for a long time, Recurrent Neural Networks, or RNNs, particularly LSTMs and GRUs, were the go-to choice for building these Sequence-to-Sequence components. Let's delve into how a Sequence-to-Sequence model works when powered by RNNs.\n",
            "As a recap, recall that RNNs are designed to process sequences by maintaining a hidden state that is updated at each step, incorporating information from the current input and the previous hidden state. This makes them naturally suited for handling sequential data like text.\n",
            "In a Sequence-to-Sequence model with RNNs, the Encoder is typically a multi-layer LSTM network. It reads the input sequence word by word (or token by token). At each time step, the encoder processes the current input token and the hidden state from the previous step to update its internal state. After processing the entire input sequence, ONLY the final hidden state of the encoder is taken as the fixed-size context vector representing the input sentence.\n",
            "Remember, this context vector is the numerical representation of the input sequence. \n",
            " \n",
            "It is then passed to the Decoder. The Decoder is another RNN (usually also an LSTM). It takes this context vector as its initial hidden state. The decoder's task is to generate the output sequence, one token at a time.\n",
            " \n",
            "While this RNN-based architecture was a breakthrough for NMT and other sequence tasks, it has significant drawbacks. The primary issue is the fixed-size context vector. For short sentences, this might be sufficient. But for longer sentences, forcing the encoder to compress all the information into a single vector creates an information bottleneck. The decoder has to rely solely on this single vector, which might not retain all the necessary details, especially those from the beginning of the input sequence. This often leads to performance degradation on longer sequences.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os"
      ],
      "metadata": {
        "id": "sfJcDl8XJkoK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Configure Gemini SDK\n",
        "genai.configure(api_key=api_key)"
      ],
      "metadata": {
        "id": "tcVaJd_DJp5f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the Gemini 2.5 Flash Lite preview model\n",
        "model = genai.GenerativeModel(\"gemini-2.5-flash-lite-preview-06-17\")"
      ],
      "metadata": {
        "id": "-gO1i-kkJuum"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(merged_output)\n",
        "\n",
        "# Print the result\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "vBRHmTLOJy_H",
        "outputId": "07e6b37b-75fc-485a-ef59-3cfb24635eb3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# RNN-Based Seq2Seq Models for Beginners\n",
            "\n",
            "**RNN-based Seq2Seq models** use **Recurrent Neural Networks (RNNs)**, specifically **LSTMs** and **GRUs**, for both the **encoder** and **decoder** components.\n",
            "\n",
            "The **encoder**, an RNN, processes the **input sequence** word by word, updating its **hidden state** at each step. After reading the entire input, it outputs a single **fixed-size context vector**. This vector numerically represents the input.\n",
            "\n",
            "The **decoder**, another RNN, uses this context vector as its initial **hidden state** to generate the **output sequence** one token at a time.\n",
            "\n",
            "A major drawback is the **fixed-size context vector**, which can lead to an **information bottleneck** for longer sequences, causing performance issues as details might be lost.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***hey***"
      ],
      "metadata": {
        "id": "lfBHDKicWZXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Hi***"
      ],
      "metadata": {
        "id": "GbLN3UklcfGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the response text as a markdown file\n",
        "output_file_name = \"summary1.md\"\n",
        "\n",
        "try:\n",
        "    with open(output_file_name, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(response.text)\n",
        "    print(f\"Successfully saved the summary to '{output_file_name}'.\")\n",
        "except IOError as e:\n",
        "    print(f\"Error saving the summary to '{output_file_name}': {e}\")"
      ],
      "metadata": {
        "id": "0KM3XHb2J3nX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394a1f67-d64e-4059-d00b-e9d135664f1d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved the summary to 'summary1.md'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(response.text)"
      ],
      "metadata": {
        "id": "wwACVJGpKMxo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d8fc62-6434-41e0-a55b-78824bfbb305"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "772"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(transcript_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7lj6AfGCJwq",
        "outputId": "2945328e-b5db-443c-bee4-afb086985eed"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2074"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(response.text)/len(transcript_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kst2HhS2CZ-5",
        "outputId": "e65c61e8-5ad2-4115-ac82-15087143d7b0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3722275795564127"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uJJCW74Z2nLe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FuS3d9xRTpD8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}